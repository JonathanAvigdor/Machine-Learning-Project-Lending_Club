{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (2260701, 151)\n",
      "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  int_rate  installment grade sub_grade     emp_title emp_length home_ownership  annual_inc  \\\n",
      "0  68407277        NaN     3600.0       3600.0           3600.0   36 months     13.99       123.03     C        C4       leadman  10+ years       MORTGAGE     55000.0   \n",
      "1  68355089        NaN    24700.0      24700.0          24700.0   36 months     11.99       820.28     C        C1      Engineer  10+ years       MORTGAGE     65000.0   \n",
      "2  68341763        NaN    20000.0      20000.0          20000.0   60 months     10.78       432.66     B        B4  truck driver  10+ years       MORTGAGE     63000.0   \n",
      "\n",
      "  verification_status   issue_d loan_status pymnt_plan                                                url desc             purpose               title zip_code addr_state    dti  \\\n",
      "0        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN  debt_consolidation  Debt consolidation    190xx         PA   5.91   \n",
      "1        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN      small_business            Business    577xx         SD  16.06   \n",
      "2        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN    home_improvement                 NaN    605xx         IL  10.78   \n",
      "\n",
      "   delinq_2yrs earliest_cr_line  fico_range_low  fico_range_high  inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc  pub_rec  revol_bal  revol_util  \\\n",
      "0          0.0         Aug-2003           675.0            679.0             1.0                    30.0                     NaN       7.0      0.0     2765.0        29.7   \n",
      "1          1.0         Dec-1999           715.0            719.0             4.0                     6.0                     NaN      22.0      0.0    21470.0        19.2   \n",
      "2          0.0         Aug-2000           695.0            699.0             0.0                     NaN                     NaN       6.0      0.0     7869.0        56.2   \n",
      "\n",
      "   total_acc initial_list_status  out_prncp  out_prncp_inv   total_pymnt  total_pymnt_inv  total_rec_prncp  total_rec_int  total_rec_late_fee  recoveries  \\\n",
      "0       13.0                   w        0.0            0.0   4421.723917          4421.72           3600.0         821.72                 0.0         0.0   \n",
      "1       38.0                   w        0.0            0.0  25679.660000         25679.66          24700.0         979.66                 0.0         0.0   \n",
      "2       18.0                   w        0.0            0.0  22705.924294         22705.92          20000.0        2705.92                 0.0         0.0   \n",
      "\n",
      "   collection_recovery_fee last_pymnt_d  last_pymnt_amnt next_pymnt_d last_credit_pull_d  last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
      "0                      0.0     Jan-2019           122.67          NaN           Mar-2019                 564.0                560.0                         0.0   \n",
      "1                      0.0     Jun-2016           926.35          NaN           Mar-2019                 699.0                695.0                         0.0   \n",
      "2                      0.0     Jun-2017         15813.30          NaN           Mar-2019                 704.0                700.0                         0.0   \n",
      "\n",
      "   mths_since_last_major_derog  policy_code application_type  annual_inc_joint  dti_joint verification_status_joint  acc_now_delinq  tot_coll_amt  tot_cur_bal  open_acc_6m  \\\n",
      "0                         30.0          1.0       Individual               NaN        NaN                       NaN             0.0         722.0     144904.0          2.0   \n",
      "1                          NaN          1.0       Individual               NaN        NaN                       NaN             0.0           0.0     204396.0          1.0   \n",
      "2                          NaN          1.0        Joint App           71000.0      13.85              Not Verified             0.0           0.0     189699.0          0.0   \n",
      "\n",
      "   open_act_il  open_il_12m  open_il_24m  mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  open_rv_24m  max_bal_bc  all_util  total_rev_hi_lim  inq_fi  total_cu_tl  \\\n",
      "0          2.0          0.0          1.0                21.0        4981.0     36.0          3.0          3.0       722.0      34.0            9300.0     3.0          1.0   \n",
      "1          1.0          0.0          1.0                19.0       18005.0     73.0          2.0          3.0      6472.0      29.0          111800.0     0.0          0.0   \n",
      "2          1.0          0.0          4.0                19.0       10827.0     73.0          0.0          2.0      2081.0      65.0           14000.0     2.0          5.0   \n",
      "\n",
      "   inq_last_12m  acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  mo_sin_old_rev_tl_op  \\\n",
      "0           4.0                   4.0      20701.0          1506.0     37.2                       0.0          0.0               148.0                 128.0   \n",
      "1           6.0                   4.0       9733.0         57830.0     27.1                       0.0          0.0               113.0                 192.0   \n",
      "2           1.0                   6.0      31617.0          2737.0     55.9                       0.0          0.0               125.0                 184.0   \n",
      "\n",
      "   mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  mths_since_recent_bc  mths_since_recent_bc_dlq  mths_since_recent_inq  mths_since_recent_revol_delinq  num_accts_ever_120_pd  \\\n",
      "0                    3.0             3.0       1.0                   4.0                      69.0                    4.0                            69.0                    2.0   \n",
      "1                    2.0             2.0       4.0                   2.0                       NaN                    0.0                             6.0                    0.0   \n",
      "2                   14.0            14.0       5.0                 101.0                       NaN                   10.0                             NaN                    0.0   \n",
      "\n",
      "   num_actv_bc_tl  num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  num_rev_accts  num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  num_tl_30dpd  \\\n",
      "0             2.0              4.0          2.0        5.0        3.0            4.0            9.0                  4.0       7.0               0.0           0.0   \n",
      "1             5.0              5.0         13.0       17.0        6.0           20.0           27.0                  5.0      22.0               0.0           0.0   \n",
      "2             2.0              3.0          2.0        4.0        6.0            4.0            7.0                  3.0       6.0               0.0           0.0   \n",
      "\n",
      "   num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
      "0                 0.0                 3.0            76.9               0.0                   0.0        0.0         178050.0             7746.0          2400.0   \n",
      "1                 0.0                 2.0            97.4               7.7                   0.0        0.0         314017.0            39475.0         79300.0   \n",
      "2                 0.0                 0.0           100.0              50.0                   0.0        0.0         218418.0            18696.0          6200.0   \n",
      "\n",
      "   total_il_high_credit_limit  revol_bal_joint  sec_app_fico_range_low  sec_app_fico_range_high sec_app_earliest_cr_line  sec_app_inq_last_6mths  sec_app_mort_acc  \\\n",
      "0                     13734.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "1                     24667.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "2                     14877.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "\n",
      "   sec_app_open_acc  sec_app_revol_util  sec_app_open_act_il  sec_app_num_rev_accts  sec_app_chargeoff_within_12_mths  sec_app_collections_12_mths_ex_med  \\\n",
      "0               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "1               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "2               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "\n",
      "   sec_app_mths_since_last_major_derog hardship_flag hardship_type hardship_reason hardship_status  deferral_term  hardship_amount hardship_start_date hardship_end_date  \\\n",
      "0                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "1                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "2                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "\n",
      "  payment_plan_start_date  hardship_length  hardship_dpd hardship_loan_status  orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  \\\n",
      "0                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "1                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "2                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "\n",
      "   hardship_last_payment_amount disbursement_method debt_settlement_flag debt_settlement_flag_date settlement_status settlement_date  settlement_amount  settlement_percentage  \\\n",
      "0                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "1                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "2                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "\n",
      "   settlement_term  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2260701 entries, 0 to 2260700\n",
      "Columns: 151 entries, id to settlement_term\n",
      "dtypes: float64(113), object(38)\n",
      "memory usage: 2.5+ GB\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 915391 rows with invalid loan_status values.\n",
      "\n",
      "Target distribution (%):\n",
      "loan_status\n",
      "fully_paid(0)    80.04\n",
      "default(1)       19.96\n",
      "Name: proportion, dtype: float64\n",
      "Shape after target cleanup: (1345310, 151)\n",
      "Dropping optional redundant columns: ['grade']\n",
      "\n",
      "Columns dropped for high missingness (> 40%): ['member_id', 'next_pymnt_d', 'orig_projected_additional_accrued_interest', 'hardship_last_payment_amount', 'hardship_payoff_balance_amount', 'hardship_type', 'hardship_status', 'hardship_amount', 'deferral_term', 'hardship_end_date', 'hardship_dpd', 'hardship_loan_status', 'hardship_length', 'payment_plan_start_date', 'hardship_reason', 'hardship_start_date', 'sec_app_mths_since_last_major_derog', 'sec_app_revol_util', 'revol_bal_joint', 'sec_app_mort_acc', 'sec_app_inq_last_6mths', 'sec_app_earliest_cr_line', 'sec_app_open_act_il', 'sec_app_fico_range_high', 'sec_app_fico_range_low', 'sec_app_num_rev_accts', 'sec_app_collections_12_mths_ex_med', 'sec_app_chargeoff_within_12_mths', 'sec_app_open_acc', 'verification_status_joint', 'dti_joint', 'annual_inc_joint', 'settlement_percentage', 'settlement_date', 'settlement_term', 'debt_settlement_flag_date', 'settlement_amount', 'settlement_status', 'desc', 'mths_since_last_record', 'mths_since_recent_bc_dlq', 'mths_since_last_major_derog', 'mths_since_recent_revol_delinq', 'il_util', 'mths_since_rcnt_il', 'all_util', 'open_acc_6m', 'total_cu_tl', 'inq_last_12m', 'max_bal_bc', 'open_il_24m', 'open_act_il', 'open_il_12m', 'inq_fi', 'total_bal_il', 'open_rv_12m', 'open_rv_24m', 'mths_since_last_delinq']\n",
      "\n",
      "Top missing % after column drop:\n",
      " mths_since_recent_inq    12.94\n",
      "num_tl_120dpd_2m          8.73\n",
      "mo_sin_old_il_acct        7.85\n",
      "emp_title                 6.38\n",
      "emp_length                5.84\n",
      "pct_tl_nvr_dlq            5.03\n",
      "avg_cur_bal               5.02\n",
      "mo_sin_rcnt_rev_tl_op     5.02\n",
      "num_rev_accts             5.02\n",
      "mo_sin_old_rev_tl_op      5.02\n",
      "total_rev_hi_lim          5.02\n",
      "mo_sin_rcnt_tl            5.02\n",
      "num_op_rev_tl             5.02\n",
      "num_il_tl                 5.02\n",
      "num_bc_tl                 5.02\n",
      "dtype: float64\n",
      "\n",
      "Downsampling rows to ~500000 (keep fraction ≈ 0.372) with stratification...\n",
      "New shape after downsampling: (500000, 92)\n",
      "New target %: {0: 80.04, 1: 19.96}\n",
      "\n",
      "Sizes -> (350000, 91) (75000, 91) (75000, 91)\n",
      "Class % (train): {0: 80.04, 1: 19.96}\n",
      "Class % (val)  : {0: 80.04, 1: 19.96}\n",
      "Class % (test) : {0: 80.04, 1: 19.96}\n",
      "\n",
      "Detected 70 numeric and 21 categorical columns.\n",
      "scale_pos_weight (train): 4.009\n",
      "Models ready: ['LogReg', 'XGBoost']\n",
      "\n",
      "=== LogReg ===  AP(Val)=0.9897\n",
      "  recall≥0.80: thr=0.9221 | Val P=0.987, R=0.800 | Test precision=0.984, recall=0.794\n",
      "  recall≥0.85: thr=0.8785 | Val P=0.985, R=0.850 | Test precision=0.983, recall=0.844\n",
      "  recall≥0.90: thr=0.8052 | Val P=0.982, R=0.902 | Test precision=0.980, recall=0.899\n",
      "\n",
      "=== XGBoost ===  AP(Val)=0.9997\n",
      "  recall≥0.80: thr=0.6210 | Val P=1.000, R=0.997 | Test precision=1.000, recall=0.997\n",
      "  recall≥0.85: thr=0.6210 | Val P=1.000, R=0.997 | Test precision=1.000, recall=0.997\n",
      "  recall≥0.90: thr=0.6210 | Val P=1.000, R=0.997 | Test precision=1.000, recall=0.997\n",
      "\n",
      "Summary (precision at target recalls on TEST):\n",
      "LogReg | R≥0.80: P=0.984 | R≥0.85: P=0.983 | R≥0.90: P=0.980\n",
      "XGBoost | R≥0.80: P=1.000 | R≥0.85: P=1.000 | R≥0.90: P=1.000\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 1 — Setup & Load\n",
    "# =========================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# --- Load ---\n",
    "DATA_PATH = \"accepted_2007_to_2018Q4.csv\"\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "print(df_raw.head(3))\n",
    "print(df_raw.info())\n",
    "\n",
    "# =========================\n",
    "# STEP 1.1 — Ensure binary target exists (1=default, 0=paid)\n",
    "# =========================\n",
    "df = df_raw.copy()\n",
    "\n",
    "if \"loan_status\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'loan_status' in the dataset.\")\n",
    "\n",
    "# Map textual labels -> {Fully Paid:0, Charged Off:1}; otherwise coerce\n",
    "if df[\"loan_status\"].dtype == \"object\":\n",
    "    df[\"loan_status\"] = df[\"loan_status\"].map({\"Fully Paid\": 0, \"Charged Off\": 1})\n",
    "\n",
    "df[\"loan_status\"] = pd.to_numeric(df[\"loan_status\"], errors=\"coerce\").astype(\"Int64\")\n",
    "before = len(df)\n",
    "df = df[df[\"loan_status\"].isin([0, 1])].copy()\n",
    "after = len(df)\n",
    "if after < before:\n",
    "    print(f\"Dropped {before-after} rows with invalid loan_status values.\")\n",
    "df[\"loan_status\"] = df[\"loan_status\"].astype(\"int8\")\n",
    "\n",
    "print(\"\\nTarget distribution (%):\")\n",
    "print((df[\"loan_status\"].value_counts(normalize=True)*100).round(2).rename({1:\"default(1)\", 0:\"fully_paid(0)\"}))\n",
    "print(\"Shape after target cleanup:\", df.shape)\n",
    "\n",
    "# Light cleaning (cheap, no feature eng)\n",
    "if \"term\" in df.columns and df[\"term\"].dtype == \"object\":\n",
    "    df[\"term\"] = df[\"term\"].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "\n",
    "if \"int_rate\" in df.columns and df[\"int_rate\"].dtype == \"object\":\n",
    "    df[\"int_rate\"] = pd.to_numeric(df[\"int_rate\"].astype(str).str.replace(\"%\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "drop_optional = [c for c in [\"grade\"] if c in df.columns]\n",
    "if drop_optional:\n",
    "    print(\"Dropping optional redundant columns:\", drop_optional)\n",
    "    df = df.drop(columns=drop_optional)\n",
    "\n",
    "# =========================\n",
    "# STEP 2 — Drop very-missing columns (row imputation happens later in pipeline)\n",
    "# =========================\n",
    "MISS_THRESH = 0.40  # drop columns with >40% missing\n",
    "miss_ratio = df.isna().mean().sort_values(ascending=False)\n",
    "to_drop = miss_ratio[miss_ratio > MISS_THRESH].index.tolist()\n",
    "to_drop = [c for c in to_drop if c != \"loan_status\"]  # never drop target\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"\\nColumns dropped for high missingness (> {int(MISS_THRESH*100)}%): {to_drop}\")\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"\\nTop missing % after column drop:\\n\", (df.isna().mean().sort_values(ascending=False).head(15)*100).round(2))\n",
    "\n",
    "# =========================\n",
    "# STEP 3 — Optional row downsampling to 500k (stratified)\n",
    "# =========================\n",
    "RSEED = 42\n",
    "TARGET_ROWS = 500_000\n",
    "if len(df) > TARGET_ROWS:\n",
    "    frac_keep = TARGET_ROWS / len(df)\n",
    "    print(f\"\\nDownsampling rows to ~{TARGET_ROWS} (keep fraction ≈ {frac_keep:.3f}) with stratification...\")\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1-frac_keep, random_state=RSEED)\n",
    "    y_tmp = df[\"loan_status\"]\n",
    "    for keep_idx, _ in sss.split(df, y_tmp):\n",
    "        df = df.iloc[keep_idx].copy()\n",
    "    print(\"New shape after downsampling:\", df.shape)\n",
    "    print(\"New target %:\", (df[\"loan_status\"].value_counts(normalize=True)*100).round(2).to_dict())\n",
    "\n",
    "# =========================\n",
    "# STEP 4 — Split\n",
    "# =========================\n",
    "y_full = df[\"loan_status\"].astype(\"int8\")\n",
    "X_full = df.drop(columns=[\"loan_status\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_full, y_full, test_size=0.30, stratify=y_full, random_state=RSEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RSEED\n",
    ")\n",
    "\n",
    "print(\"\\nSizes ->\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Class % (train):\", (y_train.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "print(\"Class % (val)  :\", (y_val.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "print(\"Class % (test) :\", (y_test.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "\n",
    "# =========================\n",
    "# STEP 5 — Preprocessing (Impute + cast categoricals -> str + sparse OHE with rare-category capping)\n",
    "# =========================\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"default\")  # keep default outputs\n",
    "\n",
    "# Identify columns by dtype on X_full (after drops)\n",
    "num_cols = [c for c in X_full.columns if pd.api.types.is_numeric_dtype(X_full[c])]\n",
    "cat_cols = [c for c in X_full.columns if c not in num_cols]  # treat everything else as categorical\n",
    "\n",
    "print(f\"\\nDetected {len(num_cols)} numeric and {len(cat_cols)} categorical columns.\")\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler(with_mean=True)),\n",
    "    (\"to_float32\", FunctionTransformer(lambda X: X.astype(np.float32)))\n",
    "])\n",
    "\n",
    "# Keep sparse; bucket rare categories; cast to string after impute\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"to_str\", FunctionTransformer(lambda X: X.astype(str))),\n",
    "    (\"ohe\", OneHotEncoder(\n",
    "        handle_unknown=\"infrequent_if_exist\",\n",
    "        min_frequency=0.005,             # keep cats >=0.5% freq; rest -> \"infrequent\"\n",
    "        sparse_output=True,              # CRITICAL: keep sparse\n",
    "        dtype=np.float32,\n",
    "        drop=\"if_binary\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.1                 # favor sparse stacking\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# STEP 6 — Models (sparse-friendly only)\n",
    "# =========================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"xgboost not installed. Run: pip install xgboost\") from e\n",
    "\n",
    "scale_pos_weight = max(1.0, (y_train == 0).sum() / max(1, (y_train == 1).sum()))\n",
    "print(\"scale_pos_weight (train):\", round(scale_pos_weight, 3))\n",
    "\n",
    "logit_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=300,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RSEED,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=RSEED,\n",
    "        n_jobs=-1,\n",
    "        max_bin=256\n",
    "    ))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"LogReg\": logit_pipe,\n",
    "    \"XGBoost\": xgb_pipe\n",
    "}\n",
    "print(\"Models ready:\", list(models.keys()))\n",
    "\n",
    "# =========================\n",
    "# STEP 7 — Evaluation (Precision @ target recall)\n",
    "# =========================\n",
    "RECALL_TARGETS = (0.80, 0.85, 0.90)\n",
    "\n",
    "def pick_threshold_at_min_recall(y_true, p1, min_recall):\n",
    "    \"\"\"Pick threshold that achieves recall >= min_recall with max precision (on validation).\"\"\"\n",
    "    prec, rec, ths = precision_recall_curve(y_true, p1, pos_label=1)\n",
    "    idxs = np.where(rec >= min_recall)[0]\n",
    "    if len(idxs) == 0:\n",
    "        return None, None, None\n",
    "    best_i = idxs[np.argmax(prec[idxs])]\n",
    "    thr = ths[min(best_i, len(ths)-1)]   # ths has len = len(prec)-1\n",
    "    return float(thr), float(prec[best_i]), float(rec[best_i])\n",
    "\n",
    "def eval_model(name, pipe, X_train, y_train, X_val, y_val, X_test, y_test, recall_targets=RECALL_TARGETS):\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Validation for threshold selection\n",
    "    p_val = pipe.predict_proba(X_val)[:, 1]\n",
    "    ap = average_precision_score(y_val, p_val)\n",
    "    print(f\"\\n=== {name} ===  AP(Val)={ap:.4f}\")\n",
    "\n",
    "    results = {}\n",
    "    for tgt in recall_targets:\n",
    "        thr, p_at, r_at = pick_threshold_at_min_recall(y_val, p_val, tgt)\n",
    "        if thr is None:\n",
    "            print(f\"  recall≥{tgt:.2f}: not reachable.\")\n",
    "            continue\n",
    "\n",
    "        p_test = pipe.predict_proba(X_test)[:, 1]\n",
    "        yhat = (p_test >= thr).astype(\"int8\")\n",
    "        cm = confusion_matrix(y_test, yhat, labels=[1, 0])  # rows: actual 1,0 ; cols: pred 1,0\n",
    "        TP, FN, FP, TN = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "        prec_test = TP/(TP+FP) if (TP+FP) > 0 else 0.0\n",
    "        rec_test  = TP/(TP+FN) if (TP+FN) > 0 else 0.0\n",
    "\n",
    "        print(f\"  recall≥{tgt:.2f}: thr={thr:.4f} | Val P={p_at:.3f}, R={r_at:.3f} | Test precision={prec_test:.3f}, recall={rec_test:.3f}\")\n",
    "        results[tgt] = {\"thr\": thr, \"precision_test\": prec_test, \"recall_test\": rec_test, \"cm\": cm}\n",
    "    return ap, results\n",
    "\n",
    "all_results = {}\n",
    "for name, pipe in models.items():\n",
    "    ap, res = eval_model(name, pipe, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    all_results[name] = {\"AP_val\": ap, \"by_recall\": res}\n",
    "\n",
    "print(\"\\nSummary (precision at target recalls on TEST):\")\n",
    "for name, info in all_results.items():\n",
    "    row = [name]\n",
    "    for tgt in RECALL_TARGETS:\n",
    "        if tgt in info[\"by_recall\"]:\n",
    "            row.append(f\"R≥{tgt:.2f}: P={info['by_recall'][tgt]['precision_test']:.3f}\")\n",
    "        else:\n",
    "            row.append(f\"R≥{tgt:.2f}: NA\")\n",
    "    print(\" | \".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (2260701, 151)\n",
      "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  int_rate  installment grade sub_grade     emp_title emp_length home_ownership  annual_inc  \\\n",
      "0  68407277        NaN     3600.0       3600.0           3600.0   36 months     13.99       123.03     C        C4       leadman  10+ years       MORTGAGE     55000.0   \n",
      "1  68355089        NaN    24700.0      24700.0          24700.0   36 months     11.99       820.28     C        C1      Engineer  10+ years       MORTGAGE     65000.0   \n",
      "2  68341763        NaN    20000.0      20000.0          20000.0   60 months     10.78       432.66     B        B4  truck driver  10+ years       MORTGAGE     63000.0   \n",
      "\n",
      "  verification_status   issue_d loan_status pymnt_plan                                                url desc             purpose               title zip_code addr_state    dti  \\\n",
      "0        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN  debt_consolidation  Debt consolidation    190xx         PA   5.91   \n",
      "1        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN      small_business            Business    577xx         SD  16.06   \n",
      "2        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN    home_improvement                 NaN    605xx         IL  10.78   \n",
      "\n",
      "   delinq_2yrs earliest_cr_line  fico_range_low  fico_range_high  inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc  pub_rec  revol_bal  revol_util  \\\n",
      "0          0.0         Aug-2003           675.0            679.0             1.0                    30.0                     NaN       7.0      0.0     2765.0        29.7   \n",
      "1          1.0         Dec-1999           715.0            719.0             4.0                     6.0                     NaN      22.0      0.0    21470.0        19.2   \n",
      "2          0.0         Aug-2000           695.0            699.0             0.0                     NaN                     NaN       6.0      0.0     7869.0        56.2   \n",
      "\n",
      "   total_acc initial_list_status  out_prncp  out_prncp_inv   total_pymnt  total_pymnt_inv  total_rec_prncp  total_rec_int  total_rec_late_fee  recoveries  \\\n",
      "0       13.0                   w        0.0            0.0   4421.723917          4421.72           3600.0         821.72                 0.0         0.0   \n",
      "1       38.0                   w        0.0            0.0  25679.660000         25679.66          24700.0         979.66                 0.0         0.0   \n",
      "2       18.0                   w        0.0            0.0  22705.924294         22705.92          20000.0        2705.92                 0.0         0.0   \n",
      "\n",
      "   collection_recovery_fee last_pymnt_d  last_pymnt_amnt next_pymnt_d last_credit_pull_d  last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
      "0                      0.0     Jan-2019           122.67          NaN           Mar-2019                 564.0                560.0                         0.0   \n",
      "1                      0.0     Jun-2016           926.35          NaN           Mar-2019                 699.0                695.0                         0.0   \n",
      "2                      0.0     Jun-2017         15813.30          NaN           Mar-2019                 704.0                700.0                         0.0   \n",
      "\n",
      "   mths_since_last_major_derog  policy_code application_type  annual_inc_joint  dti_joint verification_status_joint  acc_now_delinq  tot_coll_amt  tot_cur_bal  open_acc_6m  \\\n",
      "0                         30.0          1.0       Individual               NaN        NaN                       NaN             0.0         722.0     144904.0          2.0   \n",
      "1                          NaN          1.0       Individual               NaN        NaN                       NaN             0.0           0.0     204396.0          1.0   \n",
      "2                          NaN          1.0        Joint App           71000.0      13.85              Not Verified             0.0           0.0     189699.0          0.0   \n",
      "\n",
      "   open_act_il  open_il_12m  open_il_24m  mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  open_rv_24m  max_bal_bc  all_util  total_rev_hi_lim  inq_fi  total_cu_tl  \\\n",
      "0          2.0          0.0          1.0                21.0        4981.0     36.0          3.0          3.0       722.0      34.0            9300.0     3.0          1.0   \n",
      "1          1.0          0.0          1.0                19.0       18005.0     73.0          2.0          3.0      6472.0      29.0          111800.0     0.0          0.0   \n",
      "2          1.0          0.0          4.0                19.0       10827.0     73.0          0.0          2.0      2081.0      65.0           14000.0     2.0          5.0   \n",
      "\n",
      "   inq_last_12m  acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  mo_sin_old_rev_tl_op  \\\n",
      "0           4.0                   4.0      20701.0          1506.0     37.2                       0.0          0.0               148.0                 128.0   \n",
      "1           6.0                   4.0       9733.0         57830.0     27.1                       0.0          0.0               113.0                 192.0   \n",
      "2           1.0                   6.0      31617.0          2737.0     55.9                       0.0          0.0               125.0                 184.0   \n",
      "\n",
      "   mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  mths_since_recent_bc  mths_since_recent_bc_dlq  mths_since_recent_inq  mths_since_recent_revol_delinq  num_accts_ever_120_pd  \\\n",
      "0                    3.0             3.0       1.0                   4.0                      69.0                    4.0                            69.0                    2.0   \n",
      "1                    2.0             2.0       4.0                   2.0                       NaN                    0.0                             6.0                    0.0   \n",
      "2                   14.0            14.0       5.0                 101.0                       NaN                   10.0                             NaN                    0.0   \n",
      "\n",
      "   num_actv_bc_tl  num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  num_rev_accts  num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  num_tl_30dpd  \\\n",
      "0             2.0              4.0          2.0        5.0        3.0            4.0            9.0                  4.0       7.0               0.0           0.0   \n",
      "1             5.0              5.0         13.0       17.0        6.0           20.0           27.0                  5.0      22.0               0.0           0.0   \n",
      "2             2.0              3.0          2.0        4.0        6.0            4.0            7.0                  3.0       6.0               0.0           0.0   \n",
      "\n",
      "   num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
      "0                 0.0                 3.0            76.9               0.0                   0.0        0.0         178050.0             7746.0          2400.0   \n",
      "1                 0.0                 2.0            97.4               7.7                   0.0        0.0         314017.0            39475.0         79300.0   \n",
      "2                 0.0                 0.0           100.0              50.0                   0.0        0.0         218418.0            18696.0          6200.0   \n",
      "\n",
      "   total_il_high_credit_limit  revol_bal_joint  sec_app_fico_range_low  sec_app_fico_range_high sec_app_earliest_cr_line  sec_app_inq_last_6mths  sec_app_mort_acc  \\\n",
      "0                     13734.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "1                     24667.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "2                     14877.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "\n",
      "   sec_app_open_acc  sec_app_revol_util  sec_app_open_act_il  sec_app_num_rev_accts  sec_app_chargeoff_within_12_mths  sec_app_collections_12_mths_ex_med  \\\n",
      "0               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "1               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "2               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "\n",
      "   sec_app_mths_since_last_major_derog hardship_flag hardship_type hardship_reason hardship_status  deferral_term  hardship_amount hardship_start_date hardship_end_date  \\\n",
      "0                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "1                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "2                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "\n",
      "  payment_plan_start_date  hardship_length  hardship_dpd hardship_loan_status  orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  \\\n",
      "0                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "1                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "2                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "\n",
      "   hardship_last_payment_amount disbursement_method debt_settlement_flag debt_settlement_flag_date settlement_status settlement_date  settlement_amount  settlement_percentage  \\\n",
      "0                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "1                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "2                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "\n",
      "   settlement_term  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2260701 entries, 0 to 2260700\n",
      "Columns: 151 entries, id to settlement_term\n",
      "dtypes: float64(113), object(38)\n",
      "memory usage: 2.5+ GB\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 915391 rows with invalid loan_status values.\n",
      "\n",
      "Target distribution (%):\n",
      "loan_status\n",
      "fully_paid(0)    80.04\n",
      "default(1)       19.96\n",
      "Name: proportion, dtype: float64\n",
      "Shape after target cleanup: (1345310, 151)\n",
      "Dropping optional redundant columns: ['grade']\n",
      "\n",
      "Dropped 0 leaky columns.\n",
      "\n",
      "Columns dropped for high missingness (> 40%): ['member_id', 'orig_projected_additional_accrued_interest', 'hardship_last_payment_amount', 'deferral_term', 'hardship_payoff_balance_amount', 'sec_app_mths_since_last_major_derog', 'sec_app_revol_util', 'revol_bal_joint', 'sec_app_fico_range_high', 'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_open_act_il', 'sec_app_fico_range_low', 'sec_app_num_rev_accts', 'sec_app_inq_last_6mths', 'sec_app_collections_12_mths_ex_med', 'sec_app_earliest_cr_line', 'sec_app_chargeoff_within_12_mths', 'verification_status_joint', 'dti_joint', 'annual_inc_joint', 'desc', 'mths_since_last_record', 'mths_since_recent_bc_dlq', 'mths_since_last_major_derog', 'mths_since_recent_revol_delinq', 'il_util', 'mths_since_rcnt_il', 'all_util', 'total_cu_tl', 'inq_last_12m', 'open_acc_6m', 'max_bal_bc', 'open_act_il', 'open_rv_24m', 'open_rv_12m', 'inq_fi', 'open_il_24m', 'total_bal_il', 'open_il_12m', 'mths_since_last_delinq']\n",
      "\n",
      "Top missing % after column drop:\n",
      " mths_since_recent_inq    12.94\n",
      "num_tl_120dpd_2m          8.73\n",
      "mo_sin_old_il_acct        7.85\n",
      "emp_title                 6.38\n",
      "emp_length                5.84\n",
      "pct_tl_nvr_dlq            5.03\n",
      "avg_cur_bal               5.02\n",
      "mo_sin_old_rev_tl_op      5.02\n",
      "mo_sin_rcnt_rev_tl_op     5.02\n",
      "num_rev_accts             5.02\n",
      "total_rev_hi_lim          5.02\n",
      "num_actv_rev_tl           5.02\n",
      "mo_sin_rcnt_tl            5.02\n",
      "num_accts_ever_120_pd     5.02\n",
      "num_actv_bc_tl            5.02\n",
      "dtype: float64\n",
      "\n",
      "Downsampling rows to ~500000 (keep fraction ≈ 0.372)...\n",
      "New shape after downsampling: (500000, 76)\n",
      "New target %: {0: 80.04, 1: 19.96}\n",
      "\n",
      "Sizes -> (350000, 75) (75000, 75) (75000, 75)\n",
      "Class % (train): {0: 80.04, 1: 19.96}\n",
      "Class % (val)  : {0: 80.04, 1: 19.96}\n",
      "Class % (test) : {0: 80.04, 1: 19.96}\n",
      "\n",
      "Detected 60 numeric and 15 categorical columns.\n",
      "scale_pos_weight (train): 4.009\n",
      "Models ready: ['LogReg', 'XGBoost']\n",
      "\n",
      "=== LogReg ===  AP(Val)=0.7942\n",
      "  recall≥0.80: thr=0.6817 | Val P=0.740, R=0.800 | Test precision=0.746, recall=0.804\n",
      "  recall≥0.85: thr=0.5911 | Val P=0.708, R=0.850 | Test precision=0.713, recall=0.850\n",
      "  recall≥0.90: thr=0.4755 | Val P=0.659, R=0.900 | Test precision=0.664, recall=0.897\n",
      "\n",
      "=== XGBoost ===  AP(Val)=0.8461\n",
      "  recall≥0.80: thr=0.7617 | Val P=0.758, R=0.800 | Test precision=0.763, recall=0.800\n",
      "  recall≥0.85: thr=0.6817 | Val P=0.719, R=0.850 | Test precision=0.724, recall=0.848\n",
      "  recall≥0.90: thr=0.5441 | Val P=0.668, R=0.900 | Test precision=0.672, recall=0.899\n",
      "\n",
      "Summary (precision at target recalls on TEST):\n",
      "LogReg | R≥0.80: P=0.746 | R≥0.85: P=0.713 | R≥0.90: P=0.664\n",
      "XGBoost | R≥0.80: P=0.763 | R≥0.85: P=0.724 | R≥0.90: P=0.672\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STEP 1 — Setup & Load\n",
    "# =========================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# --- Load ---\n",
    "DATA_PATH = \"accepted_2007_to_2018Q4.csv\"\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "print(df_raw.head(3))\n",
    "print(df_raw.info())\n",
    "\n",
    "# =========================\n",
    "# STEP 1.1 — Ensure binary target exists (1=default, 0=paid)\n",
    "# =========================\n",
    "df = df_raw.copy()\n",
    "\n",
    "if \"loan_status\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'loan_status' in the dataset.\")\n",
    "\n",
    "# Map textual labels -> {Fully Paid:0, Charged Off:1}; otherwise coerce\n",
    "if df[\"loan_status\"].dtype == \"object\":\n",
    "    df[\"loan_status\"] = df[\"loan_status\"].map({\"Fully Paid\": 0, \"Charged Off\": 1})\n",
    "\n",
    "df[\"loan_status\"] = pd.to_numeric(df[\"loan_status\"], errors=\"coerce\").astype(\"Int64\")\n",
    "before = len(df)\n",
    "df = df[df[\"loan_status\"].isin([0, 1])].copy()\n",
    "after = len(df)\n",
    "if after < before:\n",
    "    print(f\"Dropped {before-after} rows with invalid loan_status values.\")\n",
    "df[\"loan_status\"] = df[\"loan_status\"].astype(\"int8\")\n",
    "\n",
    "print(\"\\nTarget distribution (%):\")\n",
    "print((df[\"loan_status\"].value_counts(normalize=True)*100).round(2).rename({1:\"default(1)\", 0:\"fully_paid(0)\"}))\n",
    "print(\"Shape after target cleanup:\", df.shape)\n",
    "\n",
    "# Light cleaning\n",
    "if \"term\" in df.columns and df[\"term\"].dtype == \"object\":\n",
    "    df[\"term\"] = df[\"term\"].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "\n",
    "if \"int_rate\" in df.columns and df[\"int_rate\"].dtype == \"object\":\n",
    "    df[\"int_rate\"] = pd.to_numeric(df[\"int_rate\"].astype(str).str.replace(\"%\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "drop_optional = [c for c in [\"grade\"] if c in df.columns]\n",
    "if drop_optional:\n",
    "    print(\"Dropping optional redundant columns:\", drop_optional)\n",
    "    df = df.drop(columns=drop_optional)\n",
    "\n",
    "# =========================\n",
    "# STEP 2 — Drop leaky columns (post-origination info)\n",
    "# =========================\n",
    "LEAKY_COLS = [\n",
    "    # outcome / payment history\n",
    "    \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\",\n",
    "    \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\",\n",
    "    \"recoveries\", \"collection_recovery_fee\",\n",
    "    \"last_pymnt_d\", \"last_pymnt_amnt\", \"last_credit_pull_d\", \"next_pymnt_d\",\n",
    "\n",
    "    # hardship / settlement / plans\n",
    "    \"hardship_flag\", \"hardship_type\", \"hardship_reason\", \"hardship_status\",\n",
    "    \"hardship_amount\", \"hardship_start_date\", \"hardship_end_date\",\n",
    "    \"hardship_length\", \"hardship_dpd\", \"hardship_loan_status\",\n",
    "    \"payment_plan_start_date\",\n",
    "    \"debt_settlement_flag\", \"debt_settlement_flag_date\",\n",
    "    \"settlement_status\", \"settlement_date\", \"settlement_amount\",\n",
    "    \"settlement_percentage\", \"settlement_term\",\n",
    "\n",
    "    # identifiers / flags that don’t belong\n",
    "    \"url\", \"pymnt_plan\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in LEAKY_COLS if c in df.columns])\n",
    "print(f\"\\nDropped {len([c for c in LEAKY_COLS if c in df.columns])} leaky columns.\")\n",
    "\n",
    "# =========================\n",
    "# STEP 3 — Drop very-missing columns (>40% NaN)\n",
    "# =========================\n",
    "MISS_THRESH = 0.40\n",
    "miss_ratio = df.isna().mean().sort_values(ascending=False)\n",
    "to_drop = miss_ratio[miss_ratio > MISS_THRESH].index.tolist()\n",
    "to_drop = [c for c in to_drop if c != \"loan_status\"]\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"\\nColumns dropped for high missingness (> {int(MISS_THRESH*100)}%): {to_drop}\")\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"\\nTop missing % after column drop:\\n\", (df.isna().mean().sort_values(ascending=False).head(15)*100).round(2))\n",
    "\n",
    "# =========================\n",
    "# STEP 4 — Downsample to ~500k rows (stratified)\n",
    "# =========================\n",
    "RSEED = 42\n",
    "TARGET_ROWS = 500_000\n",
    "if len(df) > TARGET_ROWS:\n",
    "    frac_keep = TARGET_ROWS / len(df)\n",
    "    print(f\"\\nDownsampling rows to ~{TARGET_ROWS} (keep fraction ≈ {frac_keep:.3f})...\")\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1-frac_keep, random_state=RSEED)\n",
    "    y_tmp = df[\"loan_status\"]\n",
    "    for keep_idx, _ in sss.split(df, y_tmp):\n",
    "        df = df.iloc[keep_idx].copy()\n",
    "    print(\"New shape after downsampling:\", df.shape)\n",
    "    print(\"New target %:\", (df[\"loan_status\"].value_counts(normalize=True)*100).round(2).to_dict())\n",
    "\n",
    "    \n",
    "# ========================\n",
    "# STEP 5 — Split\n",
    "# =========================\n",
    "y_full = df[\"loan_status\"].astype(\"int8\")\n",
    "X_full = df.drop(columns=[\"loan_status\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_full, y_full, test_size=0.30, stratify=y_full, random_state=RSEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RSEED\n",
    ")\n",
    "\n",
    "print(\"\\nSizes ->\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Class % (train):\", (y_train.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "print(\"Class % (val)  :\", (y_val.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "print(\"Class % (test) :\", (y_test.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "\n",
    "# =========================\n",
    "# STEP 6 — Preprocessing (Impute + cast categoricals -> str + sparse OHE with rare-category capping)\n",
    "# =========================\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"default\")  # keep default outputs\n",
    "\n",
    "# Identify columns by dtype on X_full (after drops)\n",
    "num_cols = [c for c in X_full.columns if pd.api.types.is_numeric_dtype(X_full[c])]\n",
    "cat_cols = [c for c in X_full.columns if c not in num_cols]  # treat everything else as categorical\n",
    "\n",
    "print(f\"\\nDetected {len(num_cols)} numeric and {len(cat_cols)} categorical columns.\")\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler(with_mean=True)),\n",
    "    (\"to_float32\", FunctionTransformer(lambda X: X.astype(np.float32)))\n",
    "])\n",
    "\n",
    "# Keep sparse; bucket rare categories; cast to string after impute\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"to_str\", FunctionTransformer(lambda X: X.astype(str))),\n",
    "    (\"ohe\", OneHotEncoder(\n",
    "        handle_unknown=\"infrequent_if_exist\",\n",
    "        min_frequency=0.005,             # keep cats >=0.5% freq; rest -> \"infrequent\"\n",
    "        sparse_output=True,              # CRITICAL: keep sparse\n",
    "        dtype=np.float32,\n",
    "        drop=\"if_binary\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.1                 # favor sparse stacking\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# STEP 7 — Models (sparse-friendly only)\n",
    "# =========================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"xgboost not installed. Run: pip install xgboost\") from e\n",
    "\n",
    "scale_pos_weight = max(1.0, (y_train == 0).sum() / max(1, (y_train == 1).sum()))\n",
    "print(\"scale_pos_weight (train):\", round(scale_pos_weight, 3))\n",
    "\n",
    "logit_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=300,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RSEED,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=RSEED,\n",
    "        n_jobs=-1,\n",
    "        max_bin=256\n",
    "    ))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"LogReg\": logit_pipe,\n",
    "    \"XGBoost\": xgb_pipe\n",
    "}\n",
    "print(\"Models ready:\", list(models.keys()))\n",
    "\n",
    "# =========================\n",
    "# STEP 8 — Evaluation (Precision @ target recall)\n",
    "# =========================\n",
    "RECALL_TARGETS = (0.80, 0.85, 0.90)\n",
    "\n",
    "def pick_threshold_at_min_recall(y_true, p1, min_recall):\n",
    "    \"\"\"Pick threshold that achieves recall >= min_recall with max precision (on validation).\"\"\"\n",
    "    prec, rec, ths = precision_recall_curve(y_true, p1, pos_label=1)\n",
    "    idxs = np.where(rec >= min_recall)[0]\n",
    "    if len(idxs) == 0:\n",
    "        return None, None, None\n",
    "    best_i = idxs[np.argmax(prec[idxs])]\n",
    "    thr = ths[min(best_i, len(ths)-1)]   # ths has len = len(prec)-1\n",
    "    return float(thr), float(prec[best_i]), float(rec[best_i])\n",
    "\n",
    "def eval_model(name, pipe, X_train, y_train, X_val, y_val, X_test, y_test, recall_targets=RECALL_TARGETS):\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Validation for threshold selection\n",
    "    p_val = pipe.predict_proba(X_val)[:, 1]\n",
    "    ap = average_precision_score(y_val, p_val)\n",
    "    print(f\"\\n=== {name} ===  AP(Val)={ap:.4f}\")\n",
    "\n",
    "    results = {}\n",
    "    for tgt in recall_targets:\n",
    "        thr, p_at, r_at = pick_threshold_at_min_recall(y_val, p_val, tgt)\n",
    "        if thr is None:\n",
    "            print(f\"  recall≥{tgt:.2f}: not reachable.\")\n",
    "            continue\n",
    "\n",
    "        p_test = pipe.predict_proba(X_test)[:, 1]\n",
    "        yhat = (p_test >= thr).astype(\"int8\")\n",
    "        cm = confusion_matrix(y_test, yhat, labels=[1, 0])  # rows: actual 1,0 ; cols: pred 1,0\n",
    "        TP, FN, FP, TN = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "        prec_test = TP/(TP+FP) if (TP+FP) > 0 else 0.0\n",
    "        rec_test  = TP/(TP+FN) if (TP+FN) > 0 else 0.0\n",
    "\n",
    "        print(f\"  recall≥{tgt:.2f}: thr={thr:.4f} | Val P={p_at:.3f}, R={r_at:.3f} | Test precision={prec_test:.3f}, recall={rec_test:.3f}\")\n",
    "        results[tgt] = {\"thr\": thr, \"precision_test\": prec_test, \"recall_test\": rec_test, \"cm\": cm}\n",
    "    return ap, results\n",
    "\n",
    "all_results = {}\n",
    "for name, pipe in models.items():\n",
    "    ap, res = eval_model(name, pipe, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    all_results[name] = {\"AP_val\": ap, \"by_recall\": res}\n",
    "\n",
    "print(\"\\nSummary (precision at target recalls on TEST):\")\n",
    "for name, info in all_results.items():\n",
    "    row = [name]\n",
    "    for tgt in RECALL_TARGETS:\n",
    "        if tgt in info[\"by_recall\"]:\n",
    "            row.append(f\"R≥{tgt:.2f}: P={info['by_recall'][tgt]['precision_test']:.3f}\")\n",
    "        else:\n",
    "            row.append(f\"R≥{tgt:.2f}: NA\")\n",
    "    print(\" | \".join(row))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 LogReg features (by absolute coefficient):\n",
      "last_fico_range_high           -1.847535\n",
      "last_fico_range_low            -0.980509\n",
      "emp_title_infrequent_sklearn   -0.300951\n",
      "term                            0.259335\n",
      "id_infrequent_sklearn          -0.237214\n",
      "emp_title_Teacher               0.168807\n",
      "dti                             0.165097\n",
      "int_rate                        0.144501\n",
      "mo_sin_old_rev_tl_op            0.128806\n",
      "home_ownership_MORTGAGE        -0.125083\n",
      "dtype: float32\n",
      "\n",
      "Top 10 XGBoost features (by importance):\n",
      "last_fico_range_high            0.189705\n",
      "last_fico_range_low             0.095982\n",
      "term                            0.032643\n",
      "emp_title_Teacher               0.011906\n",
      "title_infrequent_sklearn        0.011111\n",
      "funded_amnt                     0.008602\n",
      "emp_title_infrequent_sklearn    0.006299\n",
      "application_type_Joint App      0.005438\n",
      "issue_d_infrequent_sklearn      0.005434\n",
      "loan_amnt                       0.005155\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FEATURE IMPORTANCE: LogReg + XGBoost\n",
    "# =========================\n",
    "import numpy as np\n",
    "\n",
    "def get_feature_names(preprocessor):\n",
    "    \"\"\"Extract feature names from ColumnTransformer after OHE + scaling.\"\"\"\n",
    "    num_features = preprocessor.named_transformers_[\"num\"][\"impute\"].feature_names_in_\n",
    "    cat_features = preprocessor.named_transformers_[\"cat\"][\"ohe\"].get_feature_names_out(preprocessor.transformers_[1][2])\n",
    "    return np.concatenate([num_features, cat_features])\n",
    "\n",
    "# 1) Logistic Regression\n",
    "logreg_model = models[\"LogReg\"].named_steps[\"model\"]\n",
    "feature_names = get_feature_names(models[\"LogReg\"].named_steps[\"preprocess\"])\n",
    "logreg_coefs = pd.Series(logreg_model.coef_[0], index=feature_names).sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 LogReg features (by absolute coefficient):\")\n",
    "print(logreg_coefs.head(10))\n",
    "\n",
    "# 2) XGBoost\n",
    "xgb_model = models[\"XGBoost\"].named_steps[\"model\"]\n",
    "xgb_importances = pd.Series(xgb_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 XGBoost features (by importance):\")\n",
    "print(xgb_importances.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (2260701, 151)\n",
      "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  int_rate  installment grade sub_grade     emp_title emp_length home_ownership  annual_inc  \\\n",
      "0  68407277        NaN     3600.0       3600.0           3600.0   36 months     13.99       123.03     C        C4       leadman  10+ years       MORTGAGE     55000.0   \n",
      "1  68355089        NaN    24700.0      24700.0          24700.0   36 months     11.99       820.28     C        C1      Engineer  10+ years       MORTGAGE     65000.0   \n",
      "2  68341763        NaN    20000.0      20000.0          20000.0   60 months     10.78       432.66     B        B4  truck driver  10+ years       MORTGAGE     63000.0   \n",
      "\n",
      "  verification_status   issue_d loan_status pymnt_plan                                                url desc             purpose               title zip_code addr_state    dti  \\\n",
      "0        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN  debt_consolidation  Debt consolidation    190xx         PA   5.91   \n",
      "1        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN      small_business            Business    577xx         SD  16.06   \n",
      "2        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN    home_improvement                 NaN    605xx         IL  10.78   \n",
      "\n",
      "   delinq_2yrs earliest_cr_line  fico_range_low  fico_range_high  inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc  pub_rec  revol_bal  revol_util  \\\n",
      "0          0.0         Aug-2003           675.0            679.0             1.0                    30.0                     NaN       7.0      0.0     2765.0        29.7   \n",
      "1          1.0         Dec-1999           715.0            719.0             4.0                     6.0                     NaN      22.0      0.0    21470.0        19.2   \n",
      "2          0.0         Aug-2000           695.0            699.0             0.0                     NaN                     NaN       6.0      0.0     7869.0        56.2   \n",
      "\n",
      "   total_acc initial_list_status  out_prncp  out_prncp_inv   total_pymnt  total_pymnt_inv  total_rec_prncp  total_rec_int  total_rec_late_fee  recoveries  \\\n",
      "0       13.0                   w        0.0            0.0   4421.723917          4421.72           3600.0         821.72                 0.0         0.0   \n",
      "1       38.0                   w        0.0            0.0  25679.660000         25679.66          24700.0         979.66                 0.0         0.0   \n",
      "2       18.0                   w        0.0            0.0  22705.924294         22705.92          20000.0        2705.92                 0.0         0.0   \n",
      "\n",
      "   collection_recovery_fee last_pymnt_d  last_pymnt_amnt next_pymnt_d last_credit_pull_d  last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
      "0                      0.0     Jan-2019           122.67          NaN           Mar-2019                 564.0                560.0                         0.0   \n",
      "1                      0.0     Jun-2016           926.35          NaN           Mar-2019                 699.0                695.0                         0.0   \n",
      "2                      0.0     Jun-2017         15813.30          NaN           Mar-2019                 704.0                700.0                         0.0   \n",
      "\n",
      "   mths_since_last_major_derog  policy_code application_type  annual_inc_joint  dti_joint verification_status_joint  acc_now_delinq  tot_coll_amt  tot_cur_bal  open_acc_6m  \\\n",
      "0                         30.0          1.0       Individual               NaN        NaN                       NaN             0.0         722.0     144904.0          2.0   \n",
      "1                          NaN          1.0       Individual               NaN        NaN                       NaN             0.0           0.0     204396.0          1.0   \n",
      "2                          NaN          1.0        Joint App           71000.0      13.85              Not Verified             0.0           0.0     189699.0          0.0   \n",
      "\n",
      "   open_act_il  open_il_12m  open_il_24m  mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  open_rv_24m  max_bal_bc  all_util  total_rev_hi_lim  inq_fi  total_cu_tl  \\\n",
      "0          2.0          0.0          1.0                21.0        4981.0     36.0          3.0          3.0       722.0      34.0            9300.0     3.0          1.0   \n",
      "1          1.0          0.0          1.0                19.0       18005.0     73.0          2.0          3.0      6472.0      29.0          111800.0     0.0          0.0   \n",
      "2          1.0          0.0          4.0                19.0       10827.0     73.0          0.0          2.0      2081.0      65.0           14000.0     2.0          5.0   \n",
      "\n",
      "   inq_last_12m  acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  mo_sin_old_rev_tl_op  \\\n",
      "0           4.0                   4.0      20701.0          1506.0     37.2                       0.0          0.0               148.0                 128.0   \n",
      "1           6.0                   4.0       9733.0         57830.0     27.1                       0.0          0.0               113.0                 192.0   \n",
      "2           1.0                   6.0      31617.0          2737.0     55.9                       0.0          0.0               125.0                 184.0   \n",
      "\n",
      "   mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  mths_since_recent_bc  mths_since_recent_bc_dlq  mths_since_recent_inq  mths_since_recent_revol_delinq  num_accts_ever_120_pd  \\\n",
      "0                    3.0             3.0       1.0                   4.0                      69.0                    4.0                            69.0                    2.0   \n",
      "1                    2.0             2.0       4.0                   2.0                       NaN                    0.0                             6.0                    0.0   \n",
      "2                   14.0            14.0       5.0                 101.0                       NaN                   10.0                             NaN                    0.0   \n",
      "\n",
      "   num_actv_bc_tl  num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  num_rev_accts  num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  num_tl_30dpd  \\\n",
      "0             2.0              4.0          2.0        5.0        3.0            4.0            9.0                  4.0       7.0               0.0           0.0   \n",
      "1             5.0              5.0         13.0       17.0        6.0           20.0           27.0                  5.0      22.0               0.0           0.0   \n",
      "2             2.0              3.0          2.0        4.0        6.0            4.0            7.0                  3.0       6.0               0.0           0.0   \n",
      "\n",
      "   num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
      "0                 0.0                 3.0            76.9               0.0                   0.0        0.0         178050.0             7746.0          2400.0   \n",
      "1                 0.0                 2.0            97.4               7.7                   0.0        0.0         314017.0            39475.0         79300.0   \n",
      "2                 0.0                 0.0           100.0              50.0                   0.0        0.0         218418.0            18696.0          6200.0   \n",
      "\n",
      "   total_il_high_credit_limit  revol_bal_joint  sec_app_fico_range_low  sec_app_fico_range_high sec_app_earliest_cr_line  sec_app_inq_last_6mths  sec_app_mort_acc  \\\n",
      "0                     13734.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "1                     24667.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "2                     14877.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "\n",
      "   sec_app_open_acc  sec_app_revol_util  sec_app_open_act_il  sec_app_num_rev_accts  sec_app_chargeoff_within_12_mths  sec_app_collections_12_mths_ex_med  \\\n",
      "0               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "1               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "2               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "\n",
      "   sec_app_mths_since_last_major_derog hardship_flag hardship_type hardship_reason hardship_status  deferral_term  hardship_amount hardship_start_date hardship_end_date  \\\n",
      "0                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "1                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "2                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "\n",
      "  payment_plan_start_date  hardship_length  hardship_dpd hardship_loan_status  orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  \\\n",
      "0                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "1                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "2                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "\n",
      "   hardship_last_payment_amount disbursement_method debt_settlement_flag debt_settlement_flag_date settlement_status settlement_date  settlement_amount  settlement_percentage  \\\n",
      "0                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "1                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "2                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "\n",
      "   settlement_term  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2260701 entries, 0 to 2260700\n",
      "Columns: 151 entries, id to settlement_term\n",
      "dtypes: float64(113), object(38)\n",
      "memory usage: 2.5+ GB\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 915391 rows with invalid loan_status values.\n",
      "\n",
      "Target distribution (%):\n",
      "loan_status\n",
      "fully_paid(0)    80.04\n",
      "default(1)       19.96\n",
      "Name: proportion, dtype: float64\n",
      "Shape after target cleanup: (1345310, 151)\n",
      "Dropping optional redundant columns: ['grade']\n",
      "\n",
      "Dropped 36 leaky columns: ['out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'last_credit_pull_d', 'next_pymnt_d', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'hardship_amount', 'hardship_start_date', 'hardship_end_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', 'payment_plan_start_date', 'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date', 'settlement_amount', 'settlement_percentage', 'settlement_term', 'last_fico_range_high', 'last_fico_range_low', 'url', 'pymnt_plan', 'id']\n",
      "\n",
      "Columns dropped for high missingness (> 40%): ['member_id', 'orig_projected_additional_accrued_interest', 'hardship_last_payment_amount', 'hardship_payoff_balance_amount', 'deferral_term', 'sec_app_mths_since_last_major_derog', 'sec_app_revol_util', 'revol_bal_joint', 'sec_app_fico_range_high', 'sec_app_fico_range_low', 'sec_app_num_rev_accts', 'sec_app_inq_last_6mths', 'sec_app_collections_12_mths_ex_med', 'sec_app_earliest_cr_line', 'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_open_act_il', 'sec_app_chargeoff_within_12_mths', 'verification_status_joint', 'dti_joint', 'annual_inc_joint', 'desc', 'mths_since_last_record', 'mths_since_recent_bc_dlq', 'mths_since_last_major_derog', 'mths_since_recent_revol_delinq', 'il_util', 'mths_since_rcnt_il', 'all_util', 'total_cu_tl', 'open_acc_6m', 'inq_last_12m', 'max_bal_bc', 'inq_fi', 'open_il_24m', 'total_bal_il', 'open_il_12m', 'open_act_il', 'open_rv_12m', 'open_rv_24m', 'mths_since_last_delinq']\n",
      "\n",
      "Top missing % after column drop:\n",
      " mths_since_recent_inq    12.94\n",
      "num_tl_120dpd_2m          8.73\n",
      "mo_sin_old_il_acct        7.85\n",
      "emp_title                 6.38\n",
      "emp_length                5.84\n",
      "pct_tl_nvr_dlq            5.03\n",
      "avg_cur_bal               5.02\n",
      "mo_sin_old_rev_tl_op      5.02\n",
      "num_rev_accts             5.02\n",
      "mo_sin_rcnt_rev_tl_op     5.02\n",
      "mo_sin_rcnt_tl            5.02\n",
      "num_actv_bc_tl            5.02\n",
      "num_accts_ever_120_pd     5.02\n",
      "tot_hi_cred_lim           5.02\n",
      "num_tl_op_past_12m        5.02\n",
      "dtype: float64\n",
      "\n",
      "Downsampling rows to ~500000 (keep fraction ≈ 0.372)...\n",
      "New shape after downsampling: (500000, 73)\n",
      "New target %: {0: 80.04, 1: 19.96}\n",
      "\n",
      "Sizes -> (350000, 72) (75000, 72) (75000, 72)\n",
      "Class % (train): {0: 80.04, 1: 19.96}\n",
      "Class % (val)  : {0: 80.04, 1: 19.96}\n",
      "Class % (test) : {0: 80.04, 1: 19.96}\n",
      "\n",
      "Detected 58 numeric and 14 categorical columns.\n",
      "scale_pos_weight (train): 4.009\n",
      "Models ready: ['LogReg', 'XGBoost']\n",
      "\n",
      "=== LogReg ===  AP(Val)=0.3961\n",
      "  recall≥0.80: thr=0.4245 | Val P=0.289, R=0.800 | Test precision=0.288, recall=0.802\n",
      "  recall≥0.85: thr=0.3858 | Val P=0.272, R=0.850 | Test precision=0.271, recall=0.852\n",
      "  recall≥0.90: thr=0.3398 | Val P=0.254, R=0.900 | Test precision=0.254, recall=0.902\n",
      "\n",
      "=== XGBoost ===  AP(Val)=0.4152\n",
      "  recall≥0.80: thr=0.4164 | Val P=0.298, R=0.800 | Test precision=0.294, recall=0.799\n",
      "  recall≥0.85: thr=0.3760 | Val P=0.281, R=0.850 | Test precision=0.278, recall=0.850\n",
      "  recall≥0.90: thr=0.3250 | Val P=0.261, R=0.900 | Test precision=0.259, recall=0.902\n",
      "\n",
      "Summary (precision at target recalls on TEST):\n",
      "LogReg | R≥0.80: P=0.288 | R≥0.85: P=0.271 | R≥0.90: P=0.254\n",
      "XGBoost | R≥0.80: P=0.294 | R≥0.85: P=0.278 | R≥0.90: P=0.259\n",
      "\n",
      "Top 15 LogReg features (by absolute coefficient):\n",
      "int_rate                        0.335715\n",
      "term                            0.254939\n",
      "emp_title_Teacher               0.174230\n",
      "emp_title_infrequent_sklearn   -0.173747\n",
      "dti                             0.166887\n",
      "title_infrequent_sklearn       -0.138980\n",
      "acc_open_past_24mths            0.125175\n",
      "issue_d_infrequent_sklearn     -0.125062\n",
      "issue_d_Apr-2016                0.125004\n",
      "purpose_small_business          0.125002\n",
      "issue_d_May-2016                0.125001\n",
      "issue_d_Jul-2016                0.125001\n",
      "sub_grade_A1                   -0.125000\n",
      "issue_d_Aug-2016                0.125000\n",
      "sub_grade_A3                   -0.125000\n",
      "dtype: float32\n",
      "\n",
      "Top 15 XGBoost features (by importance):\n",
      "int_rate                            0.063232\n",
      "term                                0.026783\n",
      "sub_grade_A1                        0.022705\n",
      "sub_grade_B3                        0.013335\n",
      "sub_grade_B4                        0.012699\n",
      "title_infrequent_sklearn            0.012664\n",
      "sub_grade_A3                        0.012356\n",
      "home_ownership_RENT                 0.010203\n",
      "emp_title_infrequent_sklearn        0.009740\n",
      "verification_status_Not Verified    0.009040\n",
      "mort_acc                            0.008329\n",
      "emp_title_Teacher                   0.008291\n",
      "fico_range_low                      0.007882\n",
      "issue_d_infrequent_sklearn          0.007682\n",
      "sub_grade_infrequent_sklearn        0.007530\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CLEAN TRAINING PIPELINE (no leakage)\n",
    "# =========================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# --- Load ---\n",
    "DATA_PATH = \"accepted_2007_to_2018Q4.csv\"\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "print(df_raw.head(3))\n",
    "print(df_raw.info())\n",
    "\n",
    "# =========================\n",
    "# STEP 1 — Target cleanup (1=default, 0=paid)\n",
    "# =========================\n",
    "df = df_raw.copy()\n",
    "\n",
    "if \"loan_status\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'loan_status' in the dataset.\")\n",
    "\n",
    "# Map textual labels -> {Fully Paid:0, Charged Off:1}; otherwise coerce\n",
    "if df[\"loan_status\"].dtype == \"object\":\n",
    "    df[\"loan_status\"] = df[\"loan_status\"].map({\"Fully Paid\": 0, \"Charged Off\": 1})\n",
    "\n",
    "df[\"loan_status\"] = pd.to_numeric(df[\"loan_status\"], errors=\"coerce\").astype(\"Int64\")\n",
    "before = len(df)\n",
    "df = df[df[\"loan_status\"].isin([0, 1])].copy()\n",
    "after = len(df)\n",
    "if after < before:\n",
    "    print(f\"Dropped {before-after} rows with invalid loan_status values.\")\n",
    "df[\"loan_status\"] = df[\"loan_status\"].astype(\"int8\")\n",
    "\n",
    "print(\"\\nTarget distribution (%):\")\n",
    "print((df[\"loan_status\"].value_counts(normalize=True)*100).round(2).rename({1:\"default(1)\", 0:\"fully_paid(0)\"}))\n",
    "print(\"Shape after target cleanup:\", df.shape)\n",
    "\n",
    "# =========================\n",
    "# STEP 2 — Light cleaning\n",
    "# =========================\n",
    "# \"36 months\" -> 36\n",
    "if \"term\" in df.columns and df[\"term\"].dtype == \"object\":\n",
    "    df[\"term\"] = df[\"term\"].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "\n",
    "# \"13.99%\" -> 13.99\n",
    "if \"int_rate\" in df.columns and df[\"int_rate\"].dtype == \"object\":\n",
    "    df[\"int_rate\"] = pd.to_numeric(df[\"int_rate\"].astype(str).str.replace(\"%\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "# drop redundant grade (sub_grade kept)\n",
    "drop_optional = [c for c in [\"grade\"] if c in df.columns]\n",
    "if drop_optional:\n",
    "    print(\"Dropping optional redundant columns:\", drop_optional)\n",
    "    df = df.drop(columns=drop_optional)\n",
    "\n",
    "# =========================\n",
    "# STEP 3 — Drop leaky columns (post-origination / identifiers)\n",
    "# =========================\n",
    "LEAKY_COLS = [\n",
    "    # outcome / payment history (definitely post-origination)\n",
    "    \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\",\n",
    "    \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\",\n",
    "    \"recoveries\", \"collection_recovery_fee\",\n",
    "    \"last_pymnt_d\", \"last_pymnt_amnt\", \"last_credit_pull_d\", \"next_pymnt_d\",\n",
    "\n",
    "    # hardship / settlement / plans\n",
    "    \"hardship_flag\", \"hardship_type\", \"hardship_reason\", \"hardship_status\",\n",
    "    \"hardship_amount\", \"hardship_start_date\", \"hardship_end_date\",\n",
    "    \"hardship_length\", \"hardship_dpd\", \"hardship_loan_status\",\n",
    "    \"payment_plan_start_date\",\n",
    "    \"debt_settlement_flag\", \"debt_settlement_flag_date\",\n",
    "    \"settlement_status\", \"settlement_date\", \"settlement_amount\",\n",
    "    \"settlement_percentage\", \"settlement_term\",\n",
    "\n",
    "    # NEWLY ADDED (we saw them dominating importances)\n",
    "    \"last_fico_range_high\", \"last_fico_range_low\",\n",
    "\n",
    "    # identifiers we don't want to learn from\n",
    "    \"url\", \"pymnt_plan\", \"id\"\n",
    "]\n",
    "present_leaky = [c for c in LEAKY_COLS if c in df.columns]\n",
    "df = df.drop(columns=present_leaky)\n",
    "print(f\"\\nDropped {len(present_leaky)} leaky columns: {present_leaky}\")\n",
    "\n",
    "# =========================\n",
    "# STEP 4 — Drop very-missing columns (>40% NaN)\n",
    "# =========================\n",
    "MISS_THRESH = 0.40\n",
    "miss_ratio = df.isna().mean().sort_values(ascending=False)\n",
    "to_drop = miss_ratio[miss_ratio > MISS_THRESH].index.tolist()\n",
    "to_drop = [c for c in to_drop if c != \"loan_status\"]\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"\\nColumns dropped for high missingness (> {int(MISS_THRESH*100)}%): {to_drop}\")\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"\\nTop missing % after column drop:\\n\", (df.isna().mean().sort_values(ascending=False).head(15)*100).round(2))\n",
    "\n",
    "# =========================\n",
    "# STEP 5 — Downsample to ~500k rows (stratified)\n",
    "# =========================\n",
    "RSEED = 42\n",
    "TARGET_ROWS = 500_000\n",
    "if len(df) > TARGET_ROWS:\n",
    "    frac_keep = TARGET_ROWS / len(df)\n",
    "    print(f\"\\nDownsampling rows to ~{TARGET_ROWS} (keep fraction ≈ {frac_keep:.3f})...\")\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1-frac_keep, random_state=RSEED)\n",
    "    y_tmp = df[\"loan_status\"]\n",
    "    for keep_idx, _ in sss.split(df, y_tmp):\n",
    "        df = df.iloc[keep_idx].copy()\n",
    "    print(\"New shape after downsampling:\", df.shape)\n",
    "    print(\"New target %:\", (df[\"loan_status\"].value_counts(normalize=True)*100).round(2).to_dict())\n",
    "\n",
    "# =========================\n",
    "# STEP 6 — Split\n",
    "# =========================\n",
    "y_full = df[\"loan_status\"].astype(\"int8\")\n",
    "X_full = df.drop(columns=[\"loan_status\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_full, y_full, test_size=0.30, stratify=y_full, random_state=RSEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RSEED\n",
    ")\n",
    "\n",
    "print(\"\\nSizes ->\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Class % (train):\", (y_train.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "print(\"Class % (val)  :\", (y_val.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "print(\"Class % (test) :\", (y_test.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "\n",
    "# =========================\n",
    "# STEP 7 — Preprocessing (sparse OHE + rare bucket)\n",
    "# =========================\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"default\")\n",
    "\n",
    "# Identify columns by dtype on X_full (after drops)\n",
    "num_cols = [c for c in X_full.columns if pd.api.types.is_numeric_dtype(X_full[c])]\n",
    "cat_cols = [c for c in X_full.columns if c not in num_cols]  # treat everything else as categorical\n",
    "\n",
    "print(f\"\\nDetected {len(num_cols)} numeric and {len(cat_cols)} categorical columns.\")\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler(with_mean=True)),\n",
    "    (\"to_float32\", FunctionTransformer(lambda X: X.astype(np.float32)))\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"to_str\", FunctionTransformer(lambda X: X.astype(str))),\n",
    "    (\"ohe\", OneHotEncoder(\n",
    "        handle_unknown=\"infrequent_if_exist\",\n",
    "        min_frequency=0.005,        # keep cats >=0.5% freq; others -> 'infrequent'\n",
    "        sparse_output=True,\n",
    "        dtype=np.float32,\n",
    "        drop=\"if_binary\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# STEP 8 — Models (sparse-friendly)\n",
    "# =========================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"xgboost not installed. Run: pip install xgboost\") from e\n",
    "\n",
    "scale_pos_weight = max(1.0, (y_train == 0).sum() / max(1, (y_train == 1).sum()))\n",
    "print(\"scale_pos_weight (train):\", round(scale_pos_weight, 3))\n",
    "\n",
    "logit_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=300,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RSEED,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=RSEED,\n",
    "        n_jobs=-1,\n",
    "        max_bin=256\n",
    "    ))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"LogReg\": logit_pipe,\n",
    "    \"XGBoost\": xgb_pipe\n",
    "}\n",
    "print(\"Models ready:\", list(models.keys()))\n",
    "\n",
    "# =========================\n",
    "# STEP 9 — Evaluation (Precision @ target recall)\n",
    "# =========================\n",
    "RECALL_TARGETS = (0.80, 0.85, 0.90)\n",
    "\n",
    "def pick_threshold_at_min_recall(y_true, p1, min_recall):\n",
    "    \"\"\"Pick threshold that achieves recall >= min_recall with max precision (on validation).\"\"\"\n",
    "    prec, rec, ths = precision_recall_curve(y_true, p1, pos_label=1)\n",
    "    idxs = np.where(rec >= min_recall)[0]\n",
    "    if len(idxs) == 0:\n",
    "        return None, None, None\n",
    "    best_i = idxs[np.argmax(prec[idxs])]\n",
    "    thr = ths[min(best_i, len(ths)-1)]   # ths has len = len(prec)-1\n",
    "    return float(thr), float(prec[best_i]), float(rec[best_i])\n",
    "\n",
    "def eval_model(name, pipe, X_train, y_train, X_val, y_val, X_test, y_test, recall_targets=RECALL_TARGETS):\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Validation for threshold selection\n",
    "    p_val = pipe.predict_proba(X_val)[:, 1]\n",
    "    ap = average_precision_score(y_val, p_val)\n",
    "    print(f\"\\n=== {name} ===  AP(Val)={ap:.4f}\")\n",
    "\n",
    "    results = {}\n",
    "    for tgt in recall_targets:\n",
    "        thr, p_at, r_at = pick_threshold_at_min_recall(y_val, p_val, tgt)\n",
    "        if thr is None:\n",
    "            print(f\"  recall≥{tgt:.2f}: not reachable.\")\n",
    "            continue\n",
    "\n",
    "        p_test = pipe.predict_proba(X_test)[:, 1]\n",
    "        yhat = (p_test >= thr).astype(\"int8\")\n",
    "        cm = confusion_matrix(y_test, yhat, labels=[1, 0])  # rows: actual 1,0 ; cols: pred 1,0\n",
    "        TP, FN, FP, TN = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "        prec_test = TP/(TP+FP) if (TP+FP) > 0 else 0.0\n",
    "        rec_test  = TP/(TP+FN) if (TP+FN) > 0 else 0.0\n",
    "\n",
    "        print(f\"  recall≥{tgt:.2f}: thr={thr:.4f} | Val P={p_at:.3f}, R={r_at:.3f} | Test precision={prec_test:.3f}, recall={rec_test:.3f}\")\n",
    "        results[tgt] = {\"thr\": thr, \"precision_test\": prec_test, \"recall_test\": rec_test, \"cm\": cm}\n",
    "    return ap, results\n",
    "\n",
    "all_results = {}\n",
    "for name, pipe in models.items():\n",
    "    ap, res = eval_model(name, pipe, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    all_results[name] = {\"AP_val\": ap, \"by_recall\": res}\n",
    "\n",
    "print(\"\\nSummary (precision at target recalls on TEST):\")\n",
    "for name, info in all_results.items():\n",
    "    row = [name]\n",
    "    for tgt in RECALL_TARGETS:\n",
    "        if tgt in info[\"by_recall\"]:\n",
    "            row.append(f\"R≥{tgt:.2f}: P={info['by_recall'][tgt]['precision_test']:.3f}\")\n",
    "        else:\n",
    "            row.append(f\"R≥{tgt:.2f}: NA\")\n",
    "    print(\" | \".join(row))\n",
    "\n",
    "# =========================\n",
    "# STEP 10 — Feature importance (optional sanity-check)\n",
    "# =========================\n",
    "# helper to get final feature names from the preprocessor\n",
    "def get_ohe_feature_names(preprocessor, num_cols, cat_cols):\n",
    "    num_out = np.array(num_cols, dtype=object)\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "    cat_out = ohe.get_feature_names_out(cat_cols)\n",
    "    return np.concatenate([num_out, cat_out])\n",
    "\n",
    "# Logistic Regression: top coefficients\n",
    "pre = logit_pipe.named_steps[\"preprocess\"]\n",
    "feature_names = get_ohe_feature_names(pre, num_cols, cat_cols)\n",
    "log_coefs = pd.Series(logit_pipe.named_steps[\"model\"].coef_.ravel(), index=feature_names)\n",
    "\n",
    "print(\"\\nTop 15 LogReg features (by absolute coefficient):\")\n",
    "print(log_coefs.reindex(log_coefs.abs().sort_values(ascending=False).head(15).index))\n",
    "\n",
    "# XGBoost: top importance\n",
    "pre_x = xgb_pipe.named_steps[\"preprocess\"]\n",
    "feature_names_x = get_ohe_feature_names(pre_x, num_cols, cat_cols)\n",
    "xgb_model = xgb_pipe.named_steps[\"model\"]\n",
    "xgb_import = pd.Series(xgb_model.feature_importances_, index=feature_names_x).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 XGBoost features (by importance):\")\n",
    "print(xgb_import.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 LogReg features (by absolute coefficient):\n",
      "int_rate                        0.335715\n",
      "term                            0.254939\n",
      "emp_title_Teacher               0.174230\n",
      "emp_title_infrequent_sklearn   -0.173747\n",
      "dti                             0.166887\n",
      "title_infrequent_sklearn       -0.138980\n",
      "acc_open_past_24mths            0.125175\n",
      "issue_d_infrequent_sklearn     -0.125062\n",
      "issue_d_Apr-2016                0.125004\n",
      "purpose_small_business          0.125002\n",
      "dtype: float32\n",
      "\n",
      "Top 10 XGBoost features (by importance):\n",
      "int_rate                            0.063232\n",
      "term                                0.026783\n",
      "sub_grade_A1                        0.022705\n",
      "sub_grade_B3                        0.013335\n",
      "sub_grade_B4                        0.012699\n",
      "title_infrequent_sklearn            0.012664\n",
      "sub_grade_A3                        0.012356\n",
      "home_ownership_RENT                 0.010203\n",
      "emp_title_infrequent_sklearn        0.009740\n",
      "verification_status_Not Verified    0.009040\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FEATURE IMPORTANCE: LogReg + XGBoost\n",
    "# =========================\n",
    "import numpy as np\n",
    "\n",
    "def get_feature_names(preprocessor):\n",
    "    \"\"\"Extract feature names from ColumnTransformer after OHE + scaling.\"\"\"\n",
    "    num_features = preprocessor.named_transformers_[\"num\"][\"impute\"].feature_names_in_\n",
    "    cat_features = preprocessor.named_transformers_[\"cat\"][\"ohe\"].get_feature_names_out(preprocessor.transformers_[1][2])\n",
    "    return np.concatenate([num_features, cat_features])\n",
    "\n",
    "# 1) Logistic Regression\n",
    "logreg_model = models[\"LogReg\"].named_steps[\"model\"]\n",
    "feature_names = get_feature_names(models[\"LogReg\"].named_steps[\"preprocess\"])\n",
    "logreg_coefs = pd.Series(logreg_model.coef_[0], index=feature_names).sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 LogReg features (by absolute coefficient):\")\n",
    "print(logreg_coefs.head(10))\n",
    "\n",
    "# 2) XGBoost\n",
    "xgb_model = models[\"XGBoost\"].named_steps[\"model\"]\n",
    "xgb_importances = pd.Series(xgb_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 XGBoost features (by importance):\")\n",
    "print(xgb_importances.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (2260701, 151)\n",
      "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  int_rate  installment grade sub_grade     emp_title emp_length home_ownership  annual_inc  \\\n",
      "0  68407277        NaN     3600.0       3600.0           3600.0   36 months     13.99       123.03     C        C4       leadman  10+ years       MORTGAGE     55000.0   \n",
      "1  68355089        NaN    24700.0      24700.0          24700.0   36 months     11.99       820.28     C        C1      Engineer  10+ years       MORTGAGE     65000.0   \n",
      "2  68341763        NaN    20000.0      20000.0          20000.0   60 months     10.78       432.66     B        B4  truck driver  10+ years       MORTGAGE     63000.0   \n",
      "\n",
      "  verification_status   issue_d loan_status pymnt_plan                                                url desc             purpose               title zip_code addr_state    dti  \\\n",
      "0        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN  debt_consolidation  Debt consolidation    190xx         PA   5.91   \n",
      "1        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN      small_business            Business    577xx         SD  16.06   \n",
      "2        Not Verified  Dec-2015  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...  NaN    home_improvement                 NaN    605xx         IL  10.78   \n",
      "\n",
      "   delinq_2yrs earliest_cr_line  fico_range_low  fico_range_high  inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc  pub_rec  revol_bal  revol_util  \\\n",
      "0          0.0         Aug-2003           675.0            679.0             1.0                    30.0                     NaN       7.0      0.0     2765.0        29.7   \n",
      "1          1.0         Dec-1999           715.0            719.0             4.0                     6.0                     NaN      22.0      0.0    21470.0        19.2   \n",
      "2          0.0         Aug-2000           695.0            699.0             0.0                     NaN                     NaN       6.0      0.0     7869.0        56.2   \n",
      "\n",
      "   total_acc initial_list_status  out_prncp  out_prncp_inv   total_pymnt  total_pymnt_inv  total_rec_prncp  total_rec_int  total_rec_late_fee  recoveries  \\\n",
      "0       13.0                   w        0.0            0.0   4421.723917          4421.72           3600.0         821.72                 0.0         0.0   \n",
      "1       38.0                   w        0.0            0.0  25679.660000         25679.66          24700.0         979.66                 0.0         0.0   \n",
      "2       18.0                   w        0.0            0.0  22705.924294         22705.92          20000.0        2705.92                 0.0         0.0   \n",
      "\n",
      "   collection_recovery_fee last_pymnt_d  last_pymnt_amnt next_pymnt_d last_credit_pull_d  last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
      "0                      0.0     Jan-2019           122.67          NaN           Mar-2019                 564.0                560.0                         0.0   \n",
      "1                      0.0     Jun-2016           926.35          NaN           Mar-2019                 699.0                695.0                         0.0   \n",
      "2                      0.0     Jun-2017         15813.30          NaN           Mar-2019                 704.0                700.0                         0.0   \n",
      "\n",
      "   mths_since_last_major_derog  policy_code application_type  annual_inc_joint  dti_joint verification_status_joint  acc_now_delinq  tot_coll_amt  tot_cur_bal  open_acc_6m  \\\n",
      "0                         30.0          1.0       Individual               NaN        NaN                       NaN             0.0         722.0     144904.0          2.0   \n",
      "1                          NaN          1.0       Individual               NaN        NaN                       NaN             0.0           0.0     204396.0          1.0   \n",
      "2                          NaN          1.0        Joint App           71000.0      13.85              Not Verified             0.0           0.0     189699.0          0.0   \n",
      "\n",
      "   open_act_il  open_il_12m  open_il_24m  mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  open_rv_24m  max_bal_bc  all_util  total_rev_hi_lim  inq_fi  total_cu_tl  \\\n",
      "0          2.0          0.0          1.0                21.0        4981.0     36.0          3.0          3.0       722.0      34.0            9300.0     3.0          1.0   \n",
      "1          1.0          0.0          1.0                19.0       18005.0     73.0          2.0          3.0      6472.0      29.0          111800.0     0.0          0.0   \n",
      "2          1.0          0.0          4.0                19.0       10827.0     73.0          0.0          2.0      2081.0      65.0           14000.0     2.0          5.0   \n",
      "\n",
      "   inq_last_12m  acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  mo_sin_old_rev_tl_op  \\\n",
      "0           4.0                   4.0      20701.0          1506.0     37.2                       0.0          0.0               148.0                 128.0   \n",
      "1           6.0                   4.0       9733.0         57830.0     27.1                       0.0          0.0               113.0                 192.0   \n",
      "2           1.0                   6.0      31617.0          2737.0     55.9                       0.0          0.0               125.0                 184.0   \n",
      "\n",
      "   mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  mths_since_recent_bc  mths_since_recent_bc_dlq  mths_since_recent_inq  mths_since_recent_revol_delinq  num_accts_ever_120_pd  \\\n",
      "0                    3.0             3.0       1.0                   4.0                      69.0                    4.0                            69.0                    2.0   \n",
      "1                    2.0             2.0       4.0                   2.0                       NaN                    0.0                             6.0                    0.0   \n",
      "2                   14.0            14.0       5.0                 101.0                       NaN                   10.0                             NaN                    0.0   \n",
      "\n",
      "   num_actv_bc_tl  num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  num_rev_accts  num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  num_tl_30dpd  \\\n",
      "0             2.0              4.0          2.0        5.0        3.0            4.0            9.0                  4.0       7.0               0.0           0.0   \n",
      "1             5.0              5.0         13.0       17.0        6.0           20.0           27.0                  5.0      22.0               0.0           0.0   \n",
      "2             2.0              3.0          2.0        4.0        6.0            4.0            7.0                  3.0       6.0               0.0           0.0   \n",
      "\n",
      "   num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
      "0                 0.0                 3.0            76.9               0.0                   0.0        0.0         178050.0             7746.0          2400.0   \n",
      "1                 0.0                 2.0            97.4               7.7                   0.0        0.0         314017.0            39475.0         79300.0   \n",
      "2                 0.0                 0.0           100.0              50.0                   0.0        0.0         218418.0            18696.0          6200.0   \n",
      "\n",
      "   total_il_high_credit_limit  revol_bal_joint  sec_app_fico_range_low  sec_app_fico_range_high sec_app_earliest_cr_line  sec_app_inq_last_6mths  sec_app_mort_acc  \\\n",
      "0                     13734.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "1                     24667.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "2                     14877.0              NaN                     NaN                      NaN                      NaN                     NaN               NaN   \n",
      "\n",
      "   sec_app_open_acc  sec_app_revol_util  sec_app_open_act_il  sec_app_num_rev_accts  sec_app_chargeoff_within_12_mths  sec_app_collections_12_mths_ex_med  \\\n",
      "0               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "1               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "2               NaN                 NaN                  NaN                    NaN                               NaN                                 NaN   \n",
      "\n",
      "   sec_app_mths_since_last_major_derog hardship_flag hardship_type hardship_reason hardship_status  deferral_term  hardship_amount hardship_start_date hardship_end_date  \\\n",
      "0                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "1                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "2                                  NaN             N           NaN             NaN             NaN            NaN              NaN                 NaN               NaN   \n",
      "\n",
      "  payment_plan_start_date  hardship_length  hardship_dpd hardship_loan_status  orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  \\\n",
      "0                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "1                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "2                     NaN              NaN           NaN                  NaN                                         NaN                             NaN   \n",
      "\n",
      "   hardship_last_payment_amount disbursement_method debt_settlement_flag debt_settlement_flag_date settlement_status settlement_date  settlement_amount  settlement_percentage  \\\n",
      "0                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "1                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "2                           NaN                Cash                    N                       NaN               NaN             NaN                NaN                    NaN   \n",
      "\n",
      "   settlement_term  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2260701 entries, 0 to 2260700\n",
      "Columns: 151 entries, id to settlement_term\n",
      "dtypes: float64(113), object(38)\n",
      "memory usage: 2.5+ GB\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 915391 rows with invalid loan_status values.\n",
      "\n",
      "Target distribution (%):\n",
      "loan_status\n",
      "fully_paid(0)    80.04\n",
      "default(1)       19.96\n",
      "Name: proportion, dtype: float64\n",
      "Shape after target cleanup: (1345310, 151)\n",
      "Dropping optional redundant columns: ['grade']\n",
      "\n",
      "Dropped 36 leaky columns: ['out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'last_credit_pull_d', 'next_pymnt_d', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'hardship_amount', 'hardship_start_date', 'hardship_end_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', 'payment_plan_start_date', 'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date', 'settlement_amount', 'settlement_percentage', 'settlement_term', 'last_fico_range_high', 'last_fico_range_low', 'url', 'pymnt_plan', 'id']\n",
      "\n",
      "Columns dropped for high missingness (> 40%): ['member_id', 'orig_projected_additional_accrued_interest', 'hardship_last_payment_amount', 'hardship_payoff_balance_amount', 'deferral_term', 'sec_app_mths_since_last_major_derog', 'sec_app_revol_util', 'revol_bal_joint', 'sec_app_fico_range_high', 'sec_app_fico_range_low', 'sec_app_num_rev_accts', 'sec_app_inq_last_6mths', 'sec_app_collections_12_mths_ex_med', 'sec_app_earliest_cr_line', 'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_open_act_il', 'sec_app_chargeoff_within_12_mths', 'verification_status_joint', 'dti_joint', 'annual_inc_joint', 'desc', 'mths_since_last_record', 'mths_since_recent_bc_dlq', 'mths_since_last_major_derog', 'mths_since_recent_revol_delinq', 'il_util', 'mths_since_rcnt_il', 'all_util', 'total_cu_tl', 'open_acc_6m', 'inq_last_12m', 'max_bal_bc', 'inq_fi', 'open_il_24m', 'total_bal_il', 'open_il_12m', 'open_act_il', 'open_rv_12m', 'open_rv_24m', 'mths_since_last_delinq']\n",
      "\n",
      "Top missing % after column drop:\n",
      " mths_since_recent_inq    12.94\n",
      "num_tl_120dpd_2m          8.73\n",
      "mo_sin_old_il_acct        7.85\n",
      "emp_title                 6.38\n",
      "emp_length                5.84\n",
      "pct_tl_nvr_dlq            5.03\n",
      "avg_cur_bal               5.02\n",
      "mo_sin_old_rev_tl_op      5.02\n",
      "num_rev_accts             5.02\n",
      "mo_sin_rcnt_rev_tl_op     5.02\n",
      "mo_sin_rcnt_tl            5.02\n",
      "num_actv_bc_tl            5.02\n",
      "num_accts_ever_120_pd     5.02\n",
      "tot_hi_cred_lim           5.02\n",
      "num_tl_op_past_12m        5.02\n",
      "dtype: float64\n",
      "\n",
      "Downsampling rows to ~500000 (keep fraction ≈ 0.372)...\n",
      "New shape after downsampling: (500000, 73)\n",
      "New target %: {0: 80.04, 1: 19.96}\n",
      "\n",
      "Sizes -> (350000, 72) (75000, 72) (75000, 72)\n",
      "Class % (train): {0: 80.04, 1: 19.96}\n",
      "Class % (val)  : {0: 80.04, 1: 19.96}\n",
      "Class % (test) : {0: 80.04, 1: 19.96}\n",
      "\n",
      "Detected 58 numeric and 14 categorical columns.\n",
      "scale_pos_weight (train): 4.009\n",
      "Models ready: ['LogReg', 'DecisionTree', 'RandomForest', 'XGBoost']\n",
      "\n",
      "=== LogReg ===  AP(Val)=0.3961\n",
      "  recall≥0.80: thr=0.4245 | Val P=0.289, R=0.800 | Test precision=0.288, recall=0.802\n",
      "  recall≥0.85: thr=0.3858 | Val P=0.272, R=0.850 | Test precision=0.271, recall=0.852\n",
      "  recall≥0.90: thr=0.3398 | Val P=0.254, R=0.900 | Test precision=0.254, recall=0.902\n",
      "\n",
      "=== DecisionTree ===  AP(Val)=0.3527\n",
      "  recall≥0.80: thr=0.3837 | Val P=0.264, R=0.800 | Test precision=0.263, recall=0.805\n",
      "  recall≥0.85: thr=0.3498 | Val P=0.251, R=0.851 | Test precision=0.250, recall=0.854\n",
      "  recall≥0.90: thr=0.2812 | Val P=0.237, R=0.900 | Test precision=0.237, recall=0.901\n",
      "\n",
      "=== RandomForest ===  AP(Val)=0.3946\n",
      "  recall≥0.80: thr=0.4300 | Val P=0.284, R=0.800 | Test precision=0.285, recall=0.807\n",
      "  recall≥0.85: thr=0.4010 | Val P=0.270, R=0.850 | Test precision=0.269, recall=0.854\n",
      "  recall≥0.90: thr=0.3645 | Val P=0.254, R=0.900 | Test precision=0.253, recall=0.903\n",
      "\n",
      "=== XGBoost ===  AP(Val)=0.4152\n",
      "  recall≥0.80: thr=0.4164 | Val P=0.298, R=0.800 | Test precision=0.294, recall=0.799\n",
      "  recall≥0.85: thr=0.3760 | Val P=0.281, R=0.850 | Test precision=0.278, recall=0.850\n",
      "  recall≥0.90: thr=0.3250 | Val P=0.261, R=0.900 | Test precision=0.259, recall=0.902\n",
      "\n",
      "Summary (precision at target recalls on TEST):\n",
      "LogReg | R≥0.80: P=0.288 | R≥0.85: P=0.271 | R≥0.90: P=0.254\n",
      "DecisionTree | R≥0.80: P=0.263 | R≥0.85: P=0.250 | R≥0.90: P=0.237\n",
      "RandomForest | R≥0.80: P=0.285 | R≥0.85: P=0.269 | R≥0.90: P=0.253\n",
      "XGBoost | R≥0.80: P=0.294 | R≥0.85: P=0.278 | R≥0.90: P=0.259\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CLEAN TRAINING PIPELINE + DecisionTree & RandomForest\n",
    "# =========================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# --- Load ---\n",
    "DATA_PATH = \"accepted_2007_to_2018Q4.csv\"\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "print(df_raw.head(3))\n",
    "print(df_raw.info())\n",
    "\n",
    "# =========================\n",
    "# STEP 1 — Target cleanup (1=default, 0=paid)\n",
    "# =========================\n",
    "df = df_raw.copy()\n",
    "\n",
    "if \"loan_status\" not in df.columns:\n",
    "    raise ValueError(\"Expected 'loan_status' in the dataset.\")\n",
    "\n",
    "# Map textual labels -> {Fully Paid:0, Charged Off:1}; otherwise coerce\n",
    "if df[\"loan_status\"].dtype == \"object\":\n",
    "    df[\"loan_status\"] = df[\"loan_status\"].map({\"Fully Paid\": 0, \"Charged Off\": 1})\n",
    "\n",
    "df[\"loan_status\"] = pd.to_numeric(df[\"loan_status\"], errors=\"coerce\").astype(\"Int64\")\n",
    "before = len(df)\n",
    "df = df[df[\"loan_status\"].isin([0, 1])].copy()\n",
    "after = len(df)\n",
    "if after < before:\n",
    "    print(f\"Dropped {before-after} rows with invalid loan_status values.\")\n",
    "df[\"loan_status\"] = df[\"loan_status\"].astype(\"int8\")\n",
    "\n",
    "print(\"\\nTarget distribution (%):\")\n",
    "print((df[\"loan_status\"].value_counts(normalize=True)*100).round(2).rename({1:\"default(1)\", 0:\"fully_paid(0)\"}))\n",
    "print(\"Shape after target cleanup:\", df.shape)\n",
    "\n",
    "# =========================\n",
    "# STEP 2 — Light cleaning\n",
    "# =========================\n",
    "# \"36 months\" -> 36\n",
    "if \"term\" in df.columns and df[\"term\"].dtype == \"object\":\n",
    "    df[\"term\"] = df[\"term\"].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "\n",
    "# \"13.99%\" -> 13.99\n",
    "if \"int_rate\" in df.columns and df[\"int_rate\"].dtype == \"object\":\n",
    "    df[\"int_rate\"] = pd.to_numeric(df[\"int_rate\"].astype(str).str.replace(\"%\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "# drop redundant grade (sub_grade kept)\n",
    "drop_optional = [c for c in [\"grade\"] if c in df.columns]\n",
    "if drop_optional:\n",
    "    print(\"Dropping optional redundant columns:\", drop_optional)\n",
    "    df = df.drop(columns=drop_optional)\n",
    "\n",
    "# =========================\n",
    "# STEP 3 — Drop leaky columns (post-origination / identifiers)\n",
    "# =========================\n",
    "LEAKY_COLS = [\n",
    "    # outcome / payment history\n",
    "    \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\",\n",
    "    \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\",\n",
    "    \"recoveries\", \"collection_recovery_fee\",\n",
    "    \"last_pymnt_d\", \"last_pymnt_amnt\", \"last_credit_pull_d\", \"next_pymnt_d\",\n",
    "\n",
    "    # hardship / settlement / plans\n",
    "    \"hardship_flag\", \"hardship_type\", \"hardship_reason\", \"hardship_status\",\n",
    "    \"hardship_amount\", \"hardship_start_date\", \"hardship_end_date\",\n",
    "    \"hardship_length\", \"hardship_dpd\", \"hardship_loan_status\",\n",
    "    \"payment_plan_start_date\",\n",
    "    \"debt_settlement_flag\", \"debt_settlement_flag_date\",\n",
    "    \"settlement_status\", \"settlement_date\", \"settlement_amount\",\n",
    "    \"settlement_percentage\", \"settlement_term\",\n",
    "\n",
    "    # newly identified leakage / identifiers\n",
    "    \"last_fico_range_high\", \"last_fico_range_low\",\n",
    "    \"url\", \"pymnt_plan\", \"id\"\n",
    "]\n",
    "present_leaky = [c for c in LEAKY_COLS if c in df.columns]\n",
    "df = df.drop(columns=present_leaky)\n",
    "print(f\"\\nDropped {len(present_leaky)} leaky columns: {present_leaky}\")\n",
    "\n",
    "# =========================\n",
    "# STEP 4 — Drop very-missing columns (>40% NaN)\n",
    "# =========================\n",
    "MISS_THRESH = 0.40\n",
    "miss_ratio = df.isna().mean().sort_values(ascending=False)\n",
    "to_drop = miss_ratio[miss_ratio > MISS_THRESH].index.tolist()\n",
    "to_drop = [c for c in to_drop if c != \"loan_status\"]\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"\\nColumns dropped for high missingness (> {int(MISS_THRESH*100)}%): {to_drop}\")\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"\\nTop missing % after column drop:\\n\", (df.isna().mean().sort_values(ascending=False).head(15)*100).round(2))\n",
    "\n",
    "# =========================\n",
    "# STEP 5 — Downsample to ~500k rows (stratified)\n",
    "# =========================\n",
    "RSEED = 42\n",
    "TARGET_ROWS = 500_000\n",
    "if len(df) > TARGET_ROWS:\n",
    "    frac_keep = TARGET_ROWS / len(df)\n",
    "    print(f\"\\nDownsampling rows to ~{TARGET_ROWS} (keep fraction ≈ {frac_keep:.3f})...\")\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1-frac_keep, random_state=RSEED)\n",
    "    y_tmp = df[\"loan_status\"]\n",
    "    for keep_idx, _ in sss.split(df, y_tmp):\n",
    "        df = df.iloc[keep_idx].copy()\n",
    "    print(\"New shape after downsampling:\", df.shape)\n",
    "    print(\"New target %:\", (df[\"loan_status\"].value_counts(normalize=True)*100).round(2).to_dict())\n",
    "\n",
    "# =========================\n",
    "# STEP 6 — Split\n",
    "# =========================\n",
    "y_full = df[\"loan_status\"].astype(\"int8\")\n",
    "X_full = df.drop(columns=[\"loan_status\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_full, y_full, test_size=0.30, stratify=y_full, random_state=RSEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RSEED\n",
    ")\n",
    "\n",
    "print(\"\\nSizes ->\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Class % (train):\", (y_train.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "print(\"Class % (val)  :\", (y_val.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "print(\"Class % (test) :\", (y_test.value_counts(normalize=True)*100).round(2).to_dict())\n",
    "\n",
    "# =========================\n",
    "# STEP 7 — Preprocessing (sparse OHE + rare bucket)\n",
    "# =========================\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"default\")\n",
    "\n",
    "# Identify columns by dtype on X_full (after drops)\n",
    "num_cols = [c for c in X_full.columns if pd.api.types.is_numeric_dtype(X_full[c])]\n",
    "cat_cols = [c for c in X_full.columns if c not in num_cols]  # treat everything else as categorical\n",
    "\n",
    "print(f\"\\nDetected {len(num_cols)} numeric and {len(cat_cols)} categorical columns.\")\n",
    "\n",
    "# Note: trees don't need scaling, but keeping it doesn't hurt; leaves numeric dense.\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler(with_mean=True)),\n",
    "    (\"to_float32\", FunctionTransformer(lambda X: X.astype(np.float32)))\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"to_str\", FunctionTransformer(lambda X: X.astype(str))),\n",
    "    (\"ohe\", OneHotEncoder(\n",
    "        handle_unknown=\"infrequent_if_exist\",\n",
    "        min_frequency=0.005,        # keep cats >=0.5% freq; others -> 'infrequent'\n",
    "        sparse_output=True,\n",
    "        dtype=np.float32,\n",
    "        drop=\"if_binary\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# STEP 8 — Models (now incl. DecisionTree & RandomForest)\n",
    "# =========================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"xgboost not installed. Run: pip install xgboost\") from e\n",
    "\n",
    "scale_pos_weight = max(1.0, (y_train == 0).sum() / max(1, (y_train == 1).sum()))\n",
    "print(\"scale_pos_weight (train):\", round(scale_pos_weight, 3))\n",
    "\n",
    "logit_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=300,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RSEED,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Decision Tree (conservative to control memory/overfit)\n",
    "dt_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", DecisionTreeClassifier(\n",
    "        criterion=\"gini\",\n",
    "        max_depth=12,            # tune up/down as needed\n",
    "        min_samples_leaf=50,     # larger leaf reduces overfitting and memory\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RSEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Random Forest (sparse-friendly in recent sklearn; params conservative)\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=14,            # tune as needed\n",
    "        min_samples_leaf=20,\n",
    "        max_features=\"sqrt\",\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=RSEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=RSEED,\n",
    "        n_jobs=-1,\n",
    "        max_bin=256\n",
    "    ))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"LogReg\": logit_pipe,\n",
    "    \"DecisionTree\": dt_pipe,\n",
    "    \"RandomForest\": rf_pipe,\n",
    "    \"XGBoost\": xgb_pipe\n",
    "}\n",
    "print(\"Models ready:\", list(models.keys()))\n",
    "\n",
    "# =========================\n",
    "# STEP 9 — Evaluation (Precision @ target recall)\n",
    "# =========================\n",
    "RECALL_TARGETS = (0.80, 0.85, 0.90)\n",
    "\n",
    "def pick_threshold_at_min_recall(y_true, p1, min_recall):\n",
    "    \"\"\"Pick threshold that achieves recall >= min_recall with max precision (on validation).\"\"\"\n",
    "    prec, rec, ths = precision_recall_curve(y_true, p1, pos_label=1)\n",
    "    idxs = np.where(rec >= min_recall)[0]\n",
    "    if len(idxs) == 0:\n",
    "        return None, None, None\n",
    "    best_i = idxs[np.argmax(prec[idxs])]\n",
    "    thr = ths[min(best_i, len(ths)-1)]   # ths has len = len(prec)-1\n",
    "    return float(thr), float(prec[best_i]), float(rec[best_i])\n",
    "\n",
    "def eval_model(name, pipe, X_train, y_train, X_val, y_val, X_test, y_test, recall_targets=RECALL_TARGETS):\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Validation for threshold selection\n",
    "    p_val = pipe.predict_proba(X_val)[:, 1]\n",
    "    ap = average_precision_score(y_val, p_val)\n",
    "    print(f\"\\n=== {name} ===  AP(Val)={ap:.4f}\")\n",
    "\n",
    "    results = {}\n",
    "    for tgt in recall_targets:\n",
    "        thr, p_at, r_at = pick_threshold_at_min_recall(y_val, p_val, tgt)\n",
    "        if thr is None:\n",
    "            print(f\"  recall≥{tgt:.2f}: not reachable.\")\n",
    "            continue\n",
    "\n",
    "        p_test = pipe.predict_proba(X_test)[:, 1]\n",
    "        yhat = (p_test >= thr).astype(\"int8\")\n",
    "        cm = confusion_matrix(y_test, yhat, labels=[1, 0])  # rows: actual 1,0 ; cols: pred 1,0\n",
    "        TP, FN, FP, TN = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "        prec_test = TP/(TP+FP) if (TP+FP) > 0 else 0.0\n",
    "        rec_test  = TP/(TP+FN) if (TP+FN) > 0 else 0.0\n",
    "\n",
    "        print(f\"  recall≥{tgt:.2f}: thr={thr:.4f} | Val P={p_at:.3f}, R={r_at:.3f} | Test precision={prec_test:.3f}, recall={rec_test:.3f}\")\n",
    "        results[tgt] = {\"thr\": thr, \"precision_test\": prec_test, \"recall_test\": rec_test, \"cm\": cm}\n",
    "    return ap, results\n",
    "\n",
    "all_results = {}\n",
    "for name, pipe in models.items():\n",
    "    ap, res = eval_model(name, pipe, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    all_results[name] = {\"AP_val\": ap, \"by_recall\": res}\n",
    "\n",
    "print(\"\\nSummary (precision at target recalls on TEST):\")\n",
    "for name, info in all_results.items():\n",
    "    row = [name]\n",
    "    for tgt in RECALL_TARGETS:\n",
    "        if tgt in info[\"by_recall\"]:\n",
    "            row.append(f\"R≥{tgt:.2f}: P={info['by_recall'][tgt]['precision_test']:.3f}\")\n",
    "        else:\n",
    "            row.append(f\"R≥{tgt:.2f}: NA\")\n",
    "    print(\" | \".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
